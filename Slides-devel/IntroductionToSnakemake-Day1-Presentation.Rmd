---
title: Introduction to <br> Snakemake
author: "Daniel Fischer"
date: "6 June 2022"
output:
  ioslides_presentation:
    css: style_bw.css
    logo: Luke.png
    template: template.html
  slidy_presentation: default
subtitle: Day 1
---

```{css, echo = FALSE}
.remark-slide-content {
  font-size: 28px;
  padding: 20px 80px 20px 80px;
}
.remark-code, .remark-inline-code {
  background: #f0f0f0;
}
.remark-code {
  font-size: 24px;
}
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
```

```{r setup, include=FALSE}
library("reticulate")   # Required for python code chunks
knitr::opts_chunk$set(echo = TRUE,
                      fig.align='center',
                      eval=FALSE)
```

<!-- 
  A Luke Markdown ioslides Theme.
  2022-06-01 v. 1.1
  Contact: Daniel Fischer <daniel.fischer@luke.fi>

  This is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  This is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You can find the GNU General Public License at <http://www.gnu.org/licenses/>.
--> 

# Outline for the Course

## Outline of topics
<div class="my-footer">Outline of topics</div> 

**Background**

1. Motivation for workflow managers
2. What is Snakemake?

# Background

## Motivation for workflow managers I
<div class="my-footer">Motivation for workflow managers I</div> 

<p class="comment">
"Until now, I dealt pretty well with bash!"
</p>

```{r, out.width = "600px", include=TRUE, fig.align="center", fig.cap=c(""), echo=FALSE, eval=TRUE}
knitr::include_graphics("figs/Pills.png")
```

## Motivation for workflow managers II
<div class="my-footer">Motivation for workflow managers II</div> 

There might be a few things that you asked yourself, though:

1. Is my analysis reproducible?
2. Is my analysis script reusable?
3. Can I restart a crashed analysis run?
4. How do I share my analysis with others?
5. Where do I get all the required software from?
6. How do I scale my analysis between my laptop and a HPC?

## Motivation for workflow managers III
<div class="my-footer">Motivation for workflow managers III</div> 

Using a workflow manager, you will find more or less easy answers to previous questions.

Naturally, it requires some efforts to learn how to use a workflow managers, but it pays off!

Unfortunately, the easy answers will come with the price of more complicated questions!

<p class="green">
We are here to solve also new, more complicated questions!
</p>

## Overview of workflow manager
<div class="my-footer">Overview workflow manager</div> 

```{r, out.width = "120%", include=TRUE, fig.align="left", fig.cap=c(""), echo=FALSE, eval=TRUE}
knitr::include_graphics("figs/wf-managers.png")
```

<p class="cite">
Wratten, L., Wilm, A. & Göke, J. Reproducible, scalable, and shareable analysis pipelines with bioinformatics workflow managers. Nat Methods 18, 1161–1168 (2021). https://doi.org/10.1038/s41592-021-01254-9
</p>

# Snakemake

## Why Snakemake?
<div class="my-footer">Why Snakemake?</div> 

Why to use Snakemake?

1. It is rather easy to use
2. It is based on Python
3. It uses filenames, so high readability
4. Covers almost any use-case of regular users
5. It straight forward to install

## How to install Snakemake?
<div class="my-footer">How to install Snakemake?</div> 

First, you would need to fulfill the prerequisites 

1. Make sure you have conda installed on your linux box
    
```{bash, eval=FALSE}
$ conda --version
```    

<p class="warning">
If this throws an error, proceed to install conda.
</p>

<p class="green">
If this gives you an version number, you have a working conda environment.
</p>

## How to install Conda?
<div class="my-footer">How to install Conda?</div> 
    
Follow the steps provided here:
<p class="link">
https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html
</p>

Essentially, you need to download the software like this:

```{bash, eval=FALSE}
$ wget https://repo.anaconda.com/miniconda/\
       Miniconda3-py39_4.12.0-Linux-x86_64.sh
```

And then run the installation script

```{bash, eval=FALSE}
$ bash Miniconda3-py39_4.12.0-Linux-x86_64.sh
```    

## Install Snakemake via Conda?
<div class="my-footer">Install Snakemake via Conda</div> 
    
If conda is available on the system, it is very easy to install conda

```{bash, eval=FALSE}
$ conda create -c conda-forge -c bioconda -n snakemake snakemake
```

Activate the newly created environment

```{bash, eval=FALSE}
$ conda activate snakemake
```

And test it

```{bash, eval=FALSE}
(snakemake)$ snakemake --version
```

## Snakemake on CSC/Puhti
<div class="my-footer">Snakemake on CSC/Puhti</div> 

* I asked from CSC support, if they could install snakemake into the default installation

* It was made available a few months ago, is not the latest version but still good enough to live with

* After login to Puhti you can load the `bioconda/3`-module and you are good to go:
```{bash, eval=FALSE}
$ module load bioconda/3
```

# Breakout: Exercise 1

# Basics of Snakemake

## Hello World I
<div class="my-footer">Hello World I</div> 

Introductions to programming typically start with an "Hello World"-example

It shows the easiest way to make the language do something. 

Let us start with the Snakemake "Hello World"-example

## Hello World II
<div class="my-footer">Hello World II</div> 

```{pyhton}
rule all:
    input: "hello-world.txt"
```

This command tells Snakemake to create a file called "hello-world.txt"

So store the above workflow as `hello-world.smk` and run it

```{bash}
snakemake --cores 1  --snakefile hello-world.smk
```

## Hello World III
<div class="my-footer">Hello World III</div> 

We will recive the following error:

```{bash}
Building DAG of jobs...
MissingInputException in line 1 of /home/daniel/course/hello-world.smk:
Missing input files for rule all:
    affected files:
        hello-world.txt
```

## Hello World IV
<div class="my-footer">Hello World IV</div> 

We noticed

* We asked Snakemake to produce a file called "hello-world.txt"
* We have *not* told, how to do it nor have we provided it

We can fix the workflow by providing this missing information

<p class="comment">
We create a rule that provides the missing information
</p>

## Hello World V
<div class="my-footer">Hello World V</div> 

The rule `say_hello` 

* does not have any `input`, hence works 'always'
* has an `output` that matches the `input` in rule `all`

```{python}
rule all:
    input: "hello-world.txt"

rule say_hello:
    output: "hello-world.txt"
    shell:"""
    echo 'Hello World!!!' > hello-world.txt
    """
```


## Hello World VI
<div class="my-footer">Hello World VI</div> 


```{bash}
Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job          count    min threads    max threads
---------  -------  -------------  -------------
say_hello        1              1              1
total            1              1              1

Select jobs to execute...
```` 

(continues on next slide)

---

```{bash}
[Wed Jun  1 10:51:02 2022]
rule say_hello:
    output: hello-world.txt
    jobid: 0
    reason: Missing output files: hello-world.txt
    resources: tmpdir=/tmp

[Wed Jun  1 10:51:02 2022]
Finished job 0.
1 of 1 steps (100%) done
Complete log: .snakemake/log/2022-06-01T105101.905046.snakemake.log
```

## Hello World VII
<div class="my-footer">Hello World VII</div> 

What would happen, if we would have mistyped our output file?

```{python}
rule all:
    input: "hello-world.txt"

rule say_hello:
    output: "hello-world.txt"
    shell:"""
    echo 'Hello World!!!' > Hello-World.txt
    """
```

## Hello World VIII
<div class="my-footer">Hello World VIII</div> 

```{bash}
Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job          count    min threads    max threads
---------  -------  -------------  -------------
say_hello        1              1              1
total            1              1              1

Select jobs to execute...
```

---

```{bash}
[Wed Jun  1 11:09:42 2022]
rule say_hello:
    output: hello-world.txt
    jobid: 0
    reason: Code has changed since last execution
    resources: tmpdir=/tmp

Waiting at most 5 seconds for missing files.
MissingOutputException in line 1 of /home/daniel/course/hello-world.smk:
Job Missing files after 5 seconds. This might be due to filesystem latency.
If that is the case, consider to increase the wait time with --latency-wait:
hello-world.txt completed successfully, but some output files are missing. 0
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2022-06-01T110941.918797.snakemake.log
```

## Hello World IX
<div class="my-footer">Hello World IX</div> 

Use directive-placeholders.

```{python}
rule all:
    input: "hello-world.txt"
    
rule say_hello:
    output: "hello-world.txt"
    shell:"""
    echo 'Hello World!!!' > {output}
    """
```

Snakemake will change occurances of `{output}` with the String given in the `output` directive.

## Hello World X - Summary
<div class="my-footer">Hello World X - Summary</div> 

* A Snakemake rule has a **name** and a number of directives, like `input`, `output` and `shell`

* `input` and `output` directives are followed by lists of files that are expected to be used or created by the rule

* In the `shell` directive, we can refer to elements of the rule via curly braces notation, e.g. by `{input}` and `{output}`

* In case we have more lines in a directive, Snakemake will concatenate them in curly braces notation, separated by a whitespace

## Unrolling the rule all I
<div class="my-footer">Unrolling the rule all I</div> 

We could run our workflow also without the `rule all`

```{python}
rule say_hello:
    output: "hello-world.txt"
    shell:"""
    echo 'Hello World!!!' > {output}
    """
```

```{bash}
snakemake --cores 1 --snakefile hello-world.smk hello-world.txt
snakemake --cores 1 --snakefile hello-world.smk say_hello
```

## Unrolling the rule all II
<div class="my-footer">Unrolling the rule all II</div> 

* You can specify a target file and Snakemake checks for the required rules to generate the file

* Snakemake also accepts rule names as targets

* If no target is given, Snakemake defines the first rule of the Snakefile as the target

* Hence, it is best practice to have a rule `all` at the top of the workflow which has all desired target files as input files

# Breakout: Exercise 2

# Wildcards

## Bioinformatics example I
<div class="my-footer">Bioinformatics example I</div> 

Lets create a basic bwa alignment workflow and store it as bwa_align.smk

```{python}
rule bwa_map:
    input:
        "data/genome.fa",
        "data/samples/A.fastq"
    output:
        "mapped_reads/A.bam"
    shell:
        "bwa mem {input} | samtools view -Sb - > {output}"
```

## Bioinformatics example II
<div class="my-footer">Bioinformatics example II</div> 

You can use Snakemake to align the fastq to the genome by running:

```{bash}
snakemake -c1 -s bwa_align.smk mapped_reads/A.bam
```

<br> <br> 
<p class="warning">
The `input` gets now a list as input (remember the `,` in the `input` directive!)
</p>


## Bioinformatics example III
<div class="my-footer">Bioinformatics example III</div> 

```{bash}
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job        count    min threads    max threads
-------  -------  -------------  -------------
bwa_map        1              1              1
total          1              1              1

Select jobs to execute...
```

(continues on the next page)
---

```{bash}
[Wed Jun  1 12:41:25 2022]
rule bwa_map:
    input: data/genome.fa, data/samples/A.fastq
    output: mapped_reads/A.bam
    jobid: 0
    resources: tmpdir=/local_scratch/fischerd

[M::bwa_idx_load_from_disk] read 0 ALT contigs
[M::process] read 25000 sequences (2525000 bp)...
[M::mem_process_seqs] Processed 25000 reads in 0.910 CPU sec, 0.911 real sec
[main] Version: 0.7.17-r1188
[main] CMD: bwa mem data/genome.fa data/samples/A.fastq
[main] Real time: 1.273 sec; CPU: 0.946 sec
[Wed Jun  1 12:41:26 2022]
Finished job 0.
1 of 1 steps (100%) done
Complete log: /users/fischerd/tmp/fischerd/.snakemake/log/2022-06-01T124123.778979.snakemake.log
```

# Exercise 3

## Wildcards I
<div class="my-footer">Wildcards I</div> 

* We used until now hard-coded filenames

* Typically, we would like to have our workflow to be universal

* We can generalise rules by using named wildcards

* That allows us to use rules universal for different files

* For that we can use `{sample}`

* Snakemake derives the correct substitute for it, based on the requested output

* It does that by going backwards, from the requested output file and changes then the remianing dependencies accordingly

## Wildcards II
<div class="my-footer">Wildcards II</div> 

The variable part of the file names are exchanged to `{sample}`

```{python}
rule bwa_map:
    input:
        "data/genome.fa",
        "data/samples/{sample}.fastq"
    output:
        "mapped_reads/{sample}.bam"
    shell:
        "bwa mem {input} | samtools view -Sb - > {output}"
```

## Wildcards III
<div class="my-footer">Wildcards III</div> 

Using the generalised rules, we can then just call the workflow as we did before

```{bash}
snakemake -c1 -s bwa_align.smk mapped_reads/C.bam
```

Also bash-magic can be used to define output files

```{bash}
snakemake -c1 -s bwa_align.smk mapped_reads/{A,B,C}.bam
```

## Wildcards IV
<div class="my-footer">Wildcards IV</div> 

```{bash}
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job        count    min threads    max threads
-------  -------  -------------  -------------
bwa_map        1              1              1
total          1              1              1

Select jobs to execute...
```

(continues on the next page)

---

```{bash}
[Wed Jun  1 13:53:38 2022]
rule bwa_map:
    input: data/genome.fa, data/samples/A.fastq
    output: mapped_reads/A.bam
    jobid: 1
    wildcards: sample=A
    resources: tmpdir=/local_scratch/fischerd
[...]
Finished job 1.
1 of 1 steps (100%) done
Complete log: /users/fischerd/tmp/fischerd/.snakemake/log/2022-06-01T135334.947264.snakemake.log
```

# Exercise 4

# Visualisation of DAG and dry-run

## DAG
<div class="my-footer">DAG</div> 

As said, the workflow is constructed using a so-called Directed acyclic graph (DAG)

This graph is calculated in runtime, based on the requested files to be generated.

We will extend our BI workflow with a few rules so that the graph is getting more interesting

## Adding a sorting step
<div class="my-footer">Adding a sorting step</div> 

Typically, after aligning data to the genome, the alignment files need to be sorted. 

````{python}
rule samtools_sort:
    input:
        "mapped_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam"
    shell:"""
      samtools sort -T sorted_reads/{wildcards.sample} \
               -O bam {input} > {output}
    """
```

Within the `shell` directive, one can access the value of the wildcard via `{wildcards.sample}`.

## Adding an indexing step
<div class="my-footer">Adding a indexing step</div> 

Further, a bam file normally needs to be indexed for faster access as well:

```{python}
rule samtools_index:
    input:
        "sorted_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam.bai"
    shell:
        "samtools index {input}"
```

## Visualising the workflow
<div class="my-footer">Visualising the workflow</div> 

We can visualise our workflow in two ways:

1. Each and every step, how our input data moves through the workflow
2. A schematic visualisation of the dependency structure of the plot

Snakemake has options to create both kinds of figures:

1. Use the option `--dag`
2. Use the option `--rulegraph`

Snakemake produces an output in DOT format (graph description language)

## Plotting the DAG
<div class="my-footer">Plotting the DAG</div> 

```{bash}
snakemake --dag -s bwa_align.smk sorted_reads/{A,B,C}.bam.bai | \
dot -Tsvg > dag.svg
```

```{r, out.width = "600px", include=TRUE, fig.align="center", fig.cap=c(""), echo=FALSE, eval=TRUE}
knitr::include_graphics("figs/dag.svg")
```


## Plotting the Rulegraph
<div class="my-footer">Plotting the DAG</div> 

```{bash}
snakemake --rulegraph -s bwa_align.smk sorted_reads/{A,B,C}.bam.bai | \
dot -Tsvg > rulegraph.svg
```

```{r, out.width = "190px", include=TRUE, fig.align="center", fig.cap=c(""), echo=FALSE, eval=TRUE}
knitr::include_graphics("figs/rulegraph.svg")
```

## Dry-run
<div class="my-footer">Dry-Run</div> 

During development and testing it is not feasible to run always the pipeline.

To test, if the DAG can be constructed, you can perform a **dry-run**

Here, the execution of the pipeline is simulated and no code is executed.

The option for that is `-n` or `--dry-run`

It is often advisable to combine the dry-run with the `-p`-option. This option
prints the generated commands also to the screen.

# Exercise 5

# The expand function

## Expand function I
<div class="my-footer">Expand-function I</div> 

It happens often that we need to use more than one sample as input.

Snakemake provides for this the helper function `expand()`

If we define e.g. the Python list

```{python}
SAMPLES = ["A", "B"]
```

we can enter it to the `expand()` function like this


## Expand function II
<div class="my-footer">Expand-function II</div> 

Using a python list `SAMPLES` in the `expand()` function

```{python}
expand("sorted_reads/{sample}.bam", sample=SAMPLES)
```

gives as output a list like this

```{python}
["sorted_reads/A.bam", "sorted_reads/B.bam"]
```

This works also via several lists as input!

## Expand function III
<div class="my-footer">Expand-function III</div> 

Using two input lists

```{python}
expand("sorted_reads/{sample}.{replicate}.bam", sample=SAMPLES, replicate=[0, 1])
```

gives all combinations in a list like this:

```{python}
["sorted_reads/A.0.bam", "sorted_reads/A.1.bam",
 "sorted_reads/B.0.bam", "sorted_reads/B.1.bam"]
```

## Variant calling
<div class="my-footer">Variant calling</div> 

Why do we need to mention both, `bam` and `bai`?

```{python}
rule bcftools_call:
    input:
        fa="data/genome.fa",
        bam=expand("sorted_reads/{sample}.bam", sample=SAMPLES),
        bai=expand("sorted_reads/{sample}.bam.bai", sample=SAMPLES)
    output:
        "calls/all.vcf"
    shell:
        "bcftools mpileup -f {input.fa} {input.bam} | "
        "bcftools call -mv - > {output}"
```

Please, notice also the named input!

# Exercise 6

# Own scripts

## Using own scripts I
<div class="my-footer">Using own scripts I</div> 

We often would like to apply also own scripts to the data, e.g to plot the quality scores of the variant calling.

We store in the following in the python script in `scripts/plot-qual.py`

```{python}
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from pysam import VariantFile

quals = [record.qual for record in VariantFile(snakemake.input[0])]
plt.hist(quals)

plt.savefig(snakemake.output[0])
```

## Using own scripts II
<div class="my-footer">Using own scripts II</div> 

And call the script then in another rule like this:

```{python}
rule plot_quals:
    input:
        "calls/all.vcf"
    output:
        "plots/quals.svg"
    script:
        "scripts/plot-quals.py"
```

## Using own scripts III
<div class="my-footer">Using own scripts III</div> 

Of course, the script could be also placed into the `shell` directly.

That is bad, as it merges the workflow logix with the script logic.

Having the scripts separately, we can share scripts between different workflows!

# Exercise 7

# Target rules
## Target rules I
<div class="my-footer">Target rules I</div> 
We learned earlier that Snakemake by default uses the first rule as target.

Hence, it is good practice to define a rule `all` that has all the desired output as `input` to generate it

In our case, this would be

```{python}
rule all:
    input:
        "plots/quals.svg"
```

## Target rules II
<div class="my-footer">Target rules II</div> 

We also said that snakemake takes rules as target in the call

That said, we can define subsets of interesting targests to generate by having different rules, e.g.

* all
* QC
* Alignment
* ...

# Exercise 8

# Multi-threading

## Specifying number of threads I
<div class="my-footer">Specifying number of threads I</div> 

Until now, we have processed our scripts only using one core. 

However, you can define the number of threads using the directive `threads`:

```{python}
rule bwa_map:
    input:
        "data/genome.fa",
        "data/samples/{sample}.fastq"
    output:
        "mapped_reads/{sample}.bam"
    threads: 8
    shell:
        "bwa mem -t {threads} {input} | samtools view -Sb - > {output}"
```

## Specifying number of threads II
<div class="my-footer">Specifying number of threads II</div> 

Of course, if you want to run a rule with a certain amount of course,
these need to be also available, otherwise the number of cores will be down-scaled.

In the workflow call, you can specify the number of cores using the `--cores` or `-c` -option.

If no number if given, all available cores are used.

```{bash}
snakemake -c10 -s bwa_align.smk
```

# Exercise 9

# Configuration files

## Configuration files I
<div class="my-footer">Configuration filesI</div> 

Until now we organised our workflow in a single file.

This, however, is not very feasible if you want to make your workflow centrally customizable.

For that reason, Snakemake provides functions to put metainformation into a configfile.

For that, you can use either YAML or JSON format.

## YAML I
<div class="my-footer">YAML I</div> 

Personally, I like YAML more, hence I will introduce only that one

TO BE CONTINUED!!!!

## Creating config.yaml file
<div class="my-footer">Creating config.yaml file</div> 

As a little reminder, these are the relevant parts in our current workflow:

```{python}
SAMPLES = ["A", "B"]

rule bcftools_call:
    input:
        fa="data/genome.fa",
        bam=expand("sorted_reads/{sample}.bam", sample=SAMPLES),
        bai=expand("sorted_reads/{sample}.bam.bai", sample=SAMPLES)
    output:
        "calls/all.vcf"
    shell:
        "bcftools mpileup -f {input.fa} {input.bam} | "
        "bcftools call -mv - > {output}"
``` 

## Creating config.yaml file II
<div class="my-footer">Creating config.yaml file II</div> 

First, we translate the python list `SAMPLES` into a yaml-file like this:

```{yaml}
samples:
    A: data/samples/A.fastq
    B: data/samples/B.fastq
```

Please note that (for later purposes), we provide also the fastq-file path for the samples

## Creating config.yaml file III
<div class="my-footer">Creating config.yaml file III</div> 

Next, we need to make this configuration also available to Snakemake.

For that, we use the directive `configfile:` like this, as first line of our snakemake workflow

```{bash}
configfile: "config.yaml"
```

Please keep in mind, that the definition for `SAMPLE` is not needed anymore.


## Creating config.yaml file IV
<div class="my-footer">Creating config.yaml file IV</div> 

Naturally, the reference to `SAMPLES` needs now to be adjusted. 

Instead of refering now to `SAMPLE`, we need to refer to the information from the configuration:

```{python}
rule bcftools_call:
    input:
        fa="data/genome.fa",
        bam=expand("sorted_reads/{sample}.bam", sample=config["samples"]),
        bai=expand("sorted_reads/{sample}.bam.bai", sample=config["samples"])
    output:
        "calls/all.vcf"
    shell:
        "bcftools mpileup -f {input.fa} {input.bam} | "
        "bcftools call -mv - > {output}"
```

## Creating config.yaml file V
<div class="my-footer">Creating config.yaml file V</div> 

In addition to the hardcoded configuration file in the workflow, snakemake offers also
an option `--configfile` where the user can specify a configuration file.

Files provided to `--configfile` will overwrite the parameters given in the workflow definition.

Providing custom configurations via this option is the common way to execute workflows.

## Creating more keys I
<div class="my-footer">Creating more keys I</div> 

Eventually, we would like eliminate all hard-coded file-paths to make the workflow as flexible as possible.

Hence, another input variable we would like to remove is the defintion of the *genome* and the alignment step:

```{python}
rule bwa_map:
    input:
        "data/genome.fa",
        "data/samples/{sample}.fastq"
    output:
        "mapped_reads/{sample}.bam"
    shell:
        "bwa mem {input} | samtools view -Sb - > {output}"
```

## Creating more keys II
<div class="my-footer">Creating more keys II</div> 

Instead of using the hardcoded path, we should refer to a key in the configuration file:

```{python}
rule bwa_map:
    input:
        config["genome"],
        "data/samples/{sample}.fastq"
    output:
        "mapped_reads/{sample}.bam"
    shell:
        "bwa mem {input} | samtools view -Sb - > {output}"
```

## Creating more keys III
<div class="my-footer">Creating more keys III</div> 

Naturally, we have to define in the configuration then, the keyword `genome`:

```{python}
genome: "data/genome.fa"

samples:
    A: data/samples/A.fastq
    B: data/samples/B.fastq
```

# Exercise 10

# Input functions

## Using input functions I
<div class="my-footer">Using iput functions I</div> 

Instead of hard-coded paths for input files you can also provide python functions.

That provides another level of generalisation and flexibility to the workflow.

You noticed, in our config file we provide already the paths to the FASTQ files from the samples, we make now use of them.

However, first we need to dive a little deeper into the way Snakemake functions.

## Snakemake execution phases I
<div class="my-footer">Snakemake execution phases I</div> 

Snakemake is executed in three phases:

1. Initialisation: Files defining the workflow are parsed and all rules are instantiated

2. DAG: DAG is built by filling wildcards and matching input and output files

3. Scheduling: DAG of jobs is executed, with jobs started according to available resources

## Snakemake execution phases II
<div class="my-footer">Snakemake execution phases II</div> 

`expand-functions are executed in the **initialisation**-phase.

Here, we do **not** know about jobs, wildcards, rule dependencies. 

<p class="warning">
We cannot determine the FASTQ-files from the config file in this phase! We do not know what jobs will be generated from this rule!
</p>
 
<p class="green">
We need to fill those in the **DAG**-phase, using Python functions
</p>

## Python functions
<div class="my-footer">Python functions</div> 

Python functions follow a basic syntax:

```{python}
def <FUNCTIONNAME>(<INPUT>)
    <BODY>
    return <OUTPUT>
```

Following that principle, we can use in our case a very basic python function to get the desired information

## Defining onput function I
<div class="my-footer">Defining onput function I</div> 

The function that reads the file path information from the corresponding key in the configuration look like this

```{python}
def get_bwa_map_input_fastqs(wildcards):
    return config["samples"][wildcards.sample]
```

* Input functions take as single argument a `wildcards` object.

* Wildcard values are accessed via attribute `wildcards.sample`

* Functions have to return a single string or a list of strings

## Defining onput function II
<div class="my-footer">Defining onput function II</div> 

Now, we can remove the hardcoded file paths and use the function instead in the rule `bwa_map:`

```{python}
rule bwa_map:
    input:
        config["genome"],
        get_bwa_map_input_fastqs
    output:
        "mapped_reads/{sample}.bam"
    threads: 8
    shell:
        "bwa mem -t {threads} {input} | samtools view -Sb - > {output}"
```

# Exercise 11

# Paramters and logging
## Parameters I
<div class="my-footer">Parameters I </div> 

Commands require often more than just `input` and `output`

These are either 

* hardcoded paramters

or 

* user-definable paramters

Snakemake uses the `params` directive to deal with parameters

## Parameters II
<div class="my-footer">Parameters II</div> 

```{python}
rule bwa_map:
    input:
        "data/genome.fa",
        get_bwa_map_input_fastqs
    output:
        "mapped_reads/{sample}.bam"
    params:
        rg=r"@RG\tID:{sample}\tSM:{sample}"
    threads: 8
    shell:
        "bwa mem -R '{params.rg}' -t {threads} {input} | samtools view -Sb - > {output}"
```

Please note the leading `r`: the string is treated as raw-string, all escaped characters are treated "as is"

## Logging I
<div class="my-footer">Logging I</div> 

As the workflow gets more complex, it is getting more important to store the output of the different function.

This should be preferable happen in individual files. For that, snakemake offers the `log`-directive.

<p class="warning">
A common mistake is to forget the `sample` keyword in the log files definition. When rules are created "per sample", also the log files need to created "per sample"
</p>

The typical way of logging is to redirect the STDERR (`2>`) into a file. Depending on the nature of the shell command, it might be also required to pipe the STDOUT (`>`) or both, the STDERR and SDTOUT  (`&>`). 

## Logging II
<div class="my-footer">Logging II</div> 

```{python}
rule bwa_map:
    input:
        "data/genome.fa",
        get_bwa_map_input_fastqs
    output:
        "mapped_reads/{sample}.bam"
    params:
        rg=r"@RG\tID:{sample}\tSM:{sample}"
    log:
        "logs/bwa_mem/{sample}.log"
    threads: 8
    shell:
        "(bwa mem -R '{params.rg}' -t {threads} {input} | "
        "samtools view -Sb - > {output}) 2> {log}"
```

## Summary - function
<div class="my-footer">Summary - function</div> 

Snakemake can provide a table of all output files and associated rules with the `--summary command`.

Also, snakemake tracks if the function source code was changed, if this happened, this will be added to the summary table as well.

In our case, we can run the summary table like this:

```{bash}
snakemake -c4 -s bwa_align.smk --configfile config3.yaml --summary
```

# Exercise 13

# Temp and protected files
## temp files I
<div class="my-footer">temp files I</div> 

By default, snakemake does not delete anything that it creates. 

For larger workflows, that can create a HUGE overhead of files that are actually not needed.

Of course, snakemake does not know, what files are of little interest for you.

For that reason, you can mark outputfiles as temporary with the `temp()` function

Once those files are not required anymore for downstream results, snakemake will delete them.

## temp files II
<div class="my-footer">temp files II</div> 

```{python}
rule bwa_map:
    input:
        config["genome"],
        get_bwa_map_input_fastqs
    output:
        temp("mapped_reads/{sample}.bam")
    params:
        rg=r"@RG\tID:{sample}\tSM:{sample}"
    log:
        "logs/bwa_mem/{sample}.log"
    threads: 8
    shell:
        "(bwa mem -R '{params.rg}' -t {threads} {input} | "
        "samtools view -Sb - > {output}) 2> {log}"
```

## Protected files I
<div class="my-footer">Protected files I</div> 

On the contrary, there might be files of utter importance that should be protected from
deleting by accident.

Snakemake can mark files to be important with the `protected()`-function.

As a consequence, these files will be marked as write-protected in the file-system and cannot be overwritten or deleted anymore by accident.

## Protected files II
<div class="my-footer">Protected files II</div> 

```{python}
rule samtools_sort:
    input:
        "mapped_reads/{sample}.bam"
    output:
        protected("sorted_reads/{sample}.bam")
    shell:
        "samtools sort -T sorted_reads/{wildcards.sample} "
        "-O bam {input} > {output}"
```

# Exercise 14

# Benchmarking
## Benchmarking I
<div class="my-footer">Benchmarking I</div> 

Snakemake can measure the wall clock time of a job.

For that, we can use the `benchmark` directive.

The directive just expects a file path, to where the benchmarking information should be stored.

## Benchmarking II
<div class="my-footer">Benchmarking II</div> 

```{python}
rule bwa_map:
    input:
        "data/genome.fa",
        lambda wildcards: config["samples"][wildcards.sample]
    output:
        temp("mapped_reads/{sample}.bam")
    benchmark:
        "benchmarks/{sample}.bwa.benchmark.txt"
    shell:
        "(bwa mem -R '{params.rg}' -t {threads} {input} | "
        "samtools view -Sb - > {output}) 2> {log}"
```

## Benchmarking III
<div class="my-footer">Benchmarking III</div> 

After adding the benchmarking directive to a rule, we can get the detailed
benchmarking information for the runtime of each rule per sample like this:

```{bash}
s       h:m:s   max_rss max_vms max_uss max_pss io_in   io_out  mean_load       cpu_time
1.2536  0:00:01 26.44   3373.65 23.18   23.39   5.49    0.00    39.89   0.87

```

This information can then be used for

* optimising the resource allocation
* Detect bottlenecks
* ...

# Exercise: Benchmarking

# Deployment of Software dependencies

# Cluster execution

# Code checker (lint)

# Job grouping

# Directory as output

# Tabular configuration

# Validation

# Remote files (S3, CSC)

# Unit tests


# Reports

## Automatic Report I
<div class="my-footer">Automatic Report I</div> 

Snakemake has the build-in feature to visualise a runtime report.

The command-line option for this is `--report`, so that the report can be created like this:

```{bash}
snakemake -c1 -s bwa_align.smk --configfile config.yaml --report
```

That way a report called report.html is created.

## Automatic Report II
<div class="my-footer">Automatic Report II</div> 



# Thank you!
 
## {.centered}
<div class="my-footer"></div> 

Please reach out if you feel you would need support or would like to join the network!

# References

---

Wratten, L., Wilm, A. & Göke, J. Reproducible, scalable, and shareable analysis pipelines with bioinformatics workflow managers. Nat Methods 18, 1161–1168 (2021). https://doi.org/10.1038/s41592-021-01254-9